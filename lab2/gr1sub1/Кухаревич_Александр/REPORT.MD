# Лабораторная работа №2: Продвинутые процессы Linux
**Студент:** Кухаревич Александр, 4 курс, 1 группа

## Цели
- Уверенно работать с жизненным циклом процессов: запуск, рестарт, корректное завершение.
- Реализовать обработку сигналов и протокол «graceful shutdown / reload».
- Управлять планированием: nice/приоритеты, CPU‑аффинити.
- Снимать и интерпретировать метрики из `/proc` и системных утилит.
- Исследовать влияние ограничений ресурсов (rlimits, опционально cgroup v2).

## Задача A)
### Постановка задачи

Необходимо реализовать супервизор (родительский процесс) и связанных с ним воркеров со следующими возможностями:

* запуск N воркеров (N ≥ 2);
* очистка «зомби-процессов» при завершении дочерних процессов;
* обработка сигналов:
  - `SIGTERM` и `SIGINT` — корректное завершение;
  - `SIGHUP` — «graceful reload» конфигурации и перезапуск воркеров;
  - `SIGUSR1` — переключение всех воркеров в режим лёгкой нагрузки;
  - `SIGUSR2` — переключение всех воркеров в режим тяжёлой нагрузки;
* перезапуск упавших воркеров с ограничением частоты рестартов;
* воркеры должны корректно реагировать на сигналы, имитировать вычислительную нагрузку и выводить диагностическую информацию о своей работе.

### 3. Структура решения

В рамках работы были разработаны две отдельные программы на C++: `supervisor` и `worker`.

#### 3.1. Супервизор

Супервизор выполняет следующие функции:

1. **Запуск воркеров.**
   При старте он считывает конфигурацию, порождает указанное количество дочерних процессов с помощью `fork()` и запускает каждую копию программы `worker` через `exec()`.

2. **Обработка завершения дочерних процессов.**
   При получении сигнала `SIGCHLD` супервизор вызывает `waitpid(..., WNOHANG)`, чтобы корректно «подобрать» завершившиеся процессы и избежать появления зомби.

3. **Корректное завершение работы.**
   При получении сигналов `SIGTERM` или `SIGINT` супервизор отправляет воркерам команду завершить работу (передаёт им `SIGTERM`) и ожидает их корректного выхода, но не дольше 5 секунд.

4. **Поддержка «graceful reload».**
   Сигнал `SIGHUP` инициирует перечитывание конфигурации и «мягкий» перезапуск всех воркеров: старые завершаются, затем запускаются новые с обновлёнными параметрами.

5. **Изменение режима работы воркеров.**
   Сигнал `SIGUSR1` переводит всех воркеров в режим лёгкой нагрузки, а `SIGUSR2` — обратно в тяжёлый режим. Реализовано широковещательно, без перезапуска процессов.

6. **Перезапуск аварийно завершившихся воркеров.**
   Если воркер завершился ненормально, супервизор автоматически создаёт новый процесс на его месте. При этом предусмотрена защита от «штормов перезапуска» — для каждого слота ведётся учёт частоты рестартов и вводится ограничение не более 5 перезапусков за 30 секунд.

#### 3.2. Воркер

Воркер является дочерним процессом и выполняет следующие функции:

1. **Имитация вычислительной нагрузки.**
   Воркер циклически выполняет псевдовычисления и делает паузы (`usleep`). Длительность работы и сна зависит от текущего режима, параметры берутся из конфигурационного файла.

2. **Поддержка двух режимов работы.**
   Реализованы режимы тяжёлой и лёгкой нагрузки. Переключение между режимами происходит динамически без перезапуска процесса — по сигналам `SIGUSR1` (лёгкий режим) и `SIGUSR2` (тяжёлый режим).

3. **Корректное завершение.**
   При получении сигнала `SIGTERM` воркер завершает цикл работы, выводит итоговую статистику (количество выполненных «тиков»), а затем корректно выходит.

4. **Диагностический вывод.**
   В процессе работы воркер периодически выводит служебную информацию: собственный PID, номер слота, текущий режим работы, количество тиков и номер CPU, на котором он выполняется (`sched_getcpu()`).

### 4. Конфигурационный файл

Файл `config.cfg` использует простой формат `KEY=VALUE`:

```ini
workers=3
mode_heavy_work_us=9000
mode_heavy_sleep_us=1000
mode_light_work_us=2000
mode_light_sleep_us=8000
restart_limit_count=5
restart_limit_window_s=30
config_reload_wait_ms=200
```

Он загружается супервизором при старте и при получении сигнала SIGHUP

### 5. Демонстрация работы
```text
Supervisor PID: 8332
  PID CMD
 8333 ./worker 0 config.cfg
 8334 ./worker 1 config.cfg
 8335 ./worker 2 config.cfg

→ LIGHT (SIGUSR1)
[worker] PID=8333 slot=0 mode=LIGHT ticks=5 cpu=3
[worker] PID=8334 slot=1 mode=LIGHT ticks=5 cpu=1
[worker] PID=8335 slot=2 mode=LIGHT ticks=5 cpu=2

→ HEAVY (SIGUSR2)
[worker] PID=8333 slot=0 mode=HEAVY ticks=10 cpu=3
[worker] PID=8334 slot=1 mode=HEAVY ticks=10 cpu=1
[worker] PID=8335 slot=2 mode=HEAVY ticks=10 cpu=2

→ Reload config (SIGHUP)
[supervisor] Reloading config and restarting workers…
[supervisor] spawned slot=0 pid=8410
[supervisor] spawned slot=1 pid=8411
[supervisor] spawned slot=2 pid=8412
[worker] PID=8410 slot=0 mode=HEAVY ticks=5 cpu=0
...

→ Graceful shutdown (SIGTERM)
[supervisor] Sending SIGTERM to workers…
[worker] PID=8410 exiting. ticks=12
[worker] PID=8411 exiting. ticks=12
[worker] PID=8412 exiting. ticks=11
[supervisor] exit
```

## Задача B):

### 1. Постановка задачи
Добавьте управление планированием для воркеров:
- Установите разный `nice` для поднабора воркеров (например, половине `+10`, остальным `0`). Сравните распределение CPU.
- Установите CPU‑аффинити (через `sched_setaffinity` или `taskset`) для разных воркеров: например, часть закрепить на 0‑м ядре, часть — на 1‑м.
- Снимите метрики `pidstat -u 1 10` и покажите, как `nice` и аффинити влияют на `%CPU`/время ожидания.

Результат: фрагменты вывода с комментариями и короткие выводы (почему так, где ограничения, как ядро распределяет квоты).

### 2. Шаги выполнения
Для демонстрации влияния приоритета `nice` и CPU-аффинити на планирование был проведён следующий эксперимент:

1. Супервизор запустил три воркера:
   - два из группы A с `nice = 0`,
   - один из группы B с `nice = +10`.
2. Группа A была привязана к ядру CPU 1, группа B – к ядру CPU 0.
3. Воркеры поочерёдно переключались между режимами `HEAVY` и `LIGHT`.
4. С помощью `pidstat` снимались показатели загрузки CPU и времени ожидания.

### 3. Результаты

**Режим HEAVY (тяжёлая нагрузка)**
Процессы группы A (без снижения приоритета) занимают почти весь CPU на своём ядре, тогда как воркер B с `nice = +10` работает с меньшей интенсивностью:

```
01:20:53   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
01:20:53  1000     11291   46.53    0.00    0.00   45.54   46.53     1  worker
01:20:53  1000     11292   89.11    0.00    0.00    0.00   89.11     0  worker
01:20:53  1000     11293   45.54    0.00    0.00   45.54   45.54     1  worker

Average:
UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
1000     11291   48.70    0.00    0.00   41.72   48.70     -  worker (groupA)
1000     11292   89.42    0.00    0.00    0.00   89.42     -  worker (groupB, nice = 10)
1000     11293   48.70    0.00    0.00   41.72   48.70     -  worker (groupA)
```

**Переключение в режим LIGHT:**
После перехода в лёгкий режим нагрузка уменьшается для всех воркеров, но соотношение по приоритетам сохраняется:

```
[worker] PID=11291 slot=0 mode=LIGHT ticks=600 cpu=1 nice=0
[worker] PID=11292 slot=1 mode=LIGHT ticks=655 cpu=0 nice=10
[worker] PID=11293 slot=2 mode=LIGHT ticks=595 cpu=1 nice=0
```

**Возврат в режим HEAVY**
После возврата в режим тяжёлой нагрузки нагрузка на воркеры с `nice=0` снова возрастает, а воркер с `nice=+10` остаётся менее приоритетным:

```
[worker] PID=11291 slot=0 mode=HEAVY ticks=860 cpu=1 nice=0
[worker] PID=11292 slot=1 mode=HEAVY ticks=940 cpu=0 nice=10
[worker] PID=11293 slot=2 mode=HEAVY ticks=860 cpu=1 nice=0
[supervisor] sending SIGTERM to workers...
[worker] PID=11293 exiting. ticks=907
[worker] PID=11292 exiting. ticks=996
[worker] PID=11291 exiting. ticks=907
```

### 4. Анализ
Эксперимент показал:

- Значение `nice` влияет на долю CPU, которую получает процесс.
- Процесс с `nice = +10` (пониженный приоритет) выполнялся медленнее, чем воркеры с `nice = 0`.
- CPU-аффинити позволило добиться предсказуемого распределения нагрузки по ядрам и исключить межъядерное вытеснение.
- При возврате в режим HEAVY воркеры без снижения приоритета снова доминируют по нагрузке, что соответствует ожиданиям.

Реализация демонстрирует корректное управление планированием процессов в Linux:
при разных значениях `nice` и заданной CPU-аффинити можно контролировать распределение ресурсов и поведение воркеров в нагруженных сценариях.

## Ключевые выводы

1. Модель "супервизор-воркер" (на C++) является надёжным способом управления группой процессов. Использование fork() и exec() позволяет полностью разделить адресные пространства, а waitpid() в обработчике SIGCHLD гарантирует отсутствие зомби-процессов.

2. Сигналы Linux — ключевой инструмент для управления жизненным циклом. Реализация обработчиков SIGTERM (для graceful shutdown), SIGHUP (для graceful reload) и SIGUSR1/SIGUSR2 (для смены режимов) позволила создать гибкую и управляемую систему.

3. Приоритеты (nice) напрямую влияют на распределение процессорного времени. Как показал эксперимент, даже при привязке к ядрам, процесс с nice = +10 получал заметно меньше CPU, чем процессы с nice = 0 на соседнем ядре, и выполнял меньше "тиков" работы.

4. CPU-аффинити (sched_setaffinity) — мощный инструмент для изоляции нагрузки. Привязка воркеров к конкретным ядрам (CPU 0 и CPU 1) позволила чётко увидеть эффект от nice и устранить "шум" от миграции процессов между ядрами, которую мог бы выполнять системный планировщик.

5. Защита от сбоев: Реализация механизма ограничения частоты рестартов (5 перезапусков за 30 секунд) важна для предотвращения "штормов перезапуска", когда сбойный воркер мог бы бесконечно перезапускаться, потребляя ресурсы.

## Ответы на вопросы задания

1. Чем процесс отличается от потока в Linux? Где это видно в ps и /proc?

Отличие: Основное различие — в разделении ресурсов. Процесс имеет собственное, изолированное адресное пространство (память), свой набор файловых дескрипторов и права. Потоки (threads) одного процесса разделяют общее адресное пространство, файловые дескрипторы и другие ресурсы, но имеют собственный стек, регистры и идентификатор (TID). Из-за этого создание потока (clone) "легче" и быстрее, чем создание процесса (fork).

Как увидеть:

ps: Команда ps -eLf показывает потоки. У всех потоков одного процесса будет одинаковый PID (ID процесса), но разные LWP (Light Weight Process, он же TID - ID потока).

/proc: Для процесса с PID=<pid> существует директория /proc/<pid>. Внутри неё есть поддиректория /proc/<pid>/task/, где для каждого потока (включая главный) создана своя директория с именем, равным его TID.

2. Как nice влияет на планирование CFS? Какие есть пределы/исключения?

Влияние: Значение nice (от -20 до +19) используется планировщиком CFS (Completely Fair Scheduler) для определения "веса" процесса. Чем ниже nice, тем выше "вес" и, соответственно, приоритет. CFS стремится дать каждому процессу долю CPU, пропорциональную его весу, относительно суммы весов всех активных процессов.

Пределы и исключения: nice работает только для стандартной политики SCHED_NORMAL. Он не влияет на процессы реального времени (SCHED_FIFO, SCHED_RR), которые всегда имеют приоритет над обычными. Эффект nice проявляется только при конкуренции за CPU. Если процессор свободен, процесс с nice = +19 всё равно получит 100% CPU.

3. Что даёт CPU-аффинити и когда она вредна?

Что даёт: CPU-аффинити (привязка процесса к ядрам CPU) повышает производительность за счёт улучшения локальности кэша. Когда процесс выполняется на одном и том же ядре, его данные с большей вероятностью остаются в кэше L1/L2 этого ядра, что снижает задержки при доступе к памяти. Это критично для высокопроизводительных вычислений (HPC) и real-time систем.

Когда вредна: Жёсткая привязка (pinning) может быть вредна. Она мешает системному планировщику сбалансировать нагрузку. Если привязать несколько "тяжёлых" процессов к одному ядру, а другие ядра оставить свободными, это приведёт к перегрузке одного ядра и простою остальных, снижая общую производительность системы.

4. Чем отличаются RLIMIT_AS, RLIMIT_DATA, RLIMIT_RSS? Почему RLIMIT_RSS часто игнорируется?

RLIMIT_AS (Address Space): Ограничивает общий объём виртуальной памяти (VMA), который может запросить процесс. Это самый надёжный и часто используемый лимит памяти.

RLIMIT_DATA: Ограничивает размер сегмента данных (heap + BSS).

RLIMIT_RSS (Resident Set Size): Попытка ограничить объём физической памяти (ОЗУ), который занимает процесс.

Почему RLIMIT_RSS игнорируется: Ядру Linux крайне сложно отслеживать и принудительно ограничивать RSS. Причины: общие библиотеки (shared memory), которые используются многими процессами; страничная подкачка (swapping), когда страницы могут выгружаться и загружаться. Управление физической памятью — глобальная задача, и ядро предпочитает использовать OOM (Out-of-Memory) Killer при общей нехватке памяти, а не RLIMIT_RSS для конкретного процесса. В современных ядрах этот лимит практически не работает.

5. Почему возможны зомби и как их избежать при массовых рестартах воркеров?

Почему возможны: Зомби — это завершившийся дочерний процесс, запись о котором (PID, код завершения) всё ещё хранится в таблице процессов. Он нужен, чтобы родительский процесс мог прочитать его статус. Зомби появляются, если родитель не вызывает wait() или waitpid() для "сбора" (reap) завершившегося дочернего процесса.

Как избежать: Родительский процесс (супервизор) обязан обрабатывать сигнал SIGCHLD, который ядро посылает ему при завершении "ребёнка". В обработчике SIGCHLD нужно в цикле вызывать waitpid(-1, &status, WNOHANG), пока он не вернёт 0. Это гарантирует "сбор" всех завершившихся детей, даже если несколько сигналов SIGCHLD "слиплись" в один. Это и было реализовано в супервизоре.

6. Чем отличается «graceful shutdown» от «graceful reload/restart»? Какие последовательности безопасны?

Graceful Shutdown (корректное завершение): Процесс (воркер) получает SIGTERM, перестаёт принимать новые задачи, завершает обработку текущих, сохраняет состояние, закрывает соединения/файлы и корректно выходит. Цель — не потерять данные.

Последовательность: Супервизор шлёт SIGTERM воркерам -> Воркеры завершают работу -> Супервизор дожидается их и выходит сам.

Graceful Reload/Restart (мягкая перезагрузка): Супервизор обновляет конфигурацию/код без остановки сервиса.

Последовательность (как в nginx): Супервизор получает SIGHUP -> Запускает новых воркеров с новой конфигурацией -> Новые воркеры готовы принимать задачи -> Супервизор шлёт SIGTERM старым воркерам -> Старые воркеры завершают свои задачи и выходят. Это обеспечивает нулевое время простоя (zero downtime).

7. Как повлияют контейнерные лимиты (cgroup v2) на наблюдаемые метрики процесса?

Контейнерные лимиты (cgroups v2) обеспечивают более строгую изоляцию, чем ulimit.

CPU: Лимит cpu.max (например, "50% одного ядра") будет строго соблюдаться. В pidstat мы увидим, что %CPU процесса не может превысить этот лимит, даже если система свободна. Может появиться ненулевое значение %steal (stolen time) — время, которое процесс хотел бы работать, но ему не дала cgroup.

Память: Лимит memory.max работает надёжно (в отличие от RLIMIT_RSS). Когда cgroup превышает лимит, OOM Killer убьёт процесс внутри этого cgroup, а не случайный процесс в системе. Это делает поведение системы предсказуемым. Метрики RSS в /proc/<pid>/status будут ограничены этим значением.

## Как проверяли

1. Проверка работоспособности C++ решения производилась с помощью системных утилит Linux в несколько этапов:

2. Запуск и мониторинг: Супервизор (./supervisor) запускался в одном терминале. В другом терминале с помощью ps -eLf | grep worker отслеживалось корректное создание воркеров, их PID и LWP (которые совпадали, т.к. воркеры однопоточные).

3. Тестирование сигналов:

`kill -SIGUSR1 <supervisor_pid>`: Проверка переключения в режим LIGHT. В логах супервизора и воркеров отслеживалось изменение режима.

`kill -SIGUSR2 <supervisor_pid>`: Возврат в режим HEAVY.

`kill -SIGHUP <supervisor_pid>`: Проверка "graceful reload". С помощью ps контролировалось, что старые PID воркеров исчезают, а новые появляются.

`kill -SIGKILL <worker_pid>`: Проверка перезапуска. Один из воркеров принудительно "убивался", и в логах супервизора отслеживалось сообщение о ненормальном завершении и запуске нового воркера на его месте.

`kill -SIGTERM <supervisor_pid>`: Проверка "graceful shutdown". Контролировалось, что воркеры выводят финальную статистику (ticks) и корректно завершаются, после чего завершается и сам супервизор.

4. Измерение метрик (Задача B):

Корректность установки nice и affinity проверялась через `ps -o pid,ni,psr,comm -u $(whoami)`.

Основное измерение производилось утилитой `pidstat -u 1 10 -p <pid1>,<pid2>,<pid3>`. Эта команда позволила в реальном времени видеть распределение %CPU между воркерами, привязанными к разным ядрам (CPU 0 и CPU 1) и с разными приоритетами (nice), что и представлено в выводах в Задаче B.

## Использование AI-инструментов
1. **Разбор теории** AI был использован для разбора теории для лабораторной работы - ознакомление с необходимой теоретической базой, объяснения команд и выдача ссылок на ознакомление с моделями, а также для предоставления базы для ответа на теоретические вопросы
2. **Генерация скелетов** AI был использован для первоначальной генерации каркасов для скриптов supervisor и worker, а также каркаса отчёта на основе условий лабороторной работы, что ускорило как начальный этап разработки, так и конечный этап - формирование отчёта по полученной программе.
3. **Помощь** AI был использован для разбора ошибок при написании, а также как помощник при заходе в тупик.
