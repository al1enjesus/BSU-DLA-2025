# Лабораторная работа №2: Продвинутые процессы Linux

**Студент:** Залещенок Евгений, 4 курс, 1 группа

**Цель работы:** Уверенно работать с жизненным циклом процессов, реализовать обработку сигналов, научиться управлять планированием процессов с помощью `nice` и `CPU-affinity`, а также исследовать влияние ограничений на ресурсы процесса и интерпретировать системные метрики из `/proc`.

---

## Часть A: Мини-супервизор с воркерами

### Описание реализации

Был реализован набор скриптов на Bash:
*   `supervisor.sh`: Процесс-родитель, который запускает N воркеров, управляет их жизненным циклом, обрабатывает сигналы (`SIGTERM`, `SIGINT`, `SIGHUP`, `SIGUSR1`, `SIGUSR2`) и перезапускает упавших воркеров. Обработка завершенных дочерних процессов (`SIGCHLD`) реализована через цикл с командой `wait -n`.
*   `worker.sh`: Дочерний процесс, который имитирует полезную нагрузку в двух режимах ("тяжелом" и "легком"). Он корректно обрабатывает `SIGTERM` для завершения и `SIGUSR1`/`SIGUSR2` для смены режима работы.
*   `config.conf`: Файл конфигурации для задания количества воркеров и параметров режимов работы.
*   `run.sh`: Сценарий для демонстрации всех функциональных возможностей супервизора и воркеров.

### Демонстрация работы

Ниже представлен полный лог выполнения скрипта `run.sh`, который демонстрирует все аспекты управления процессами.

```text
==================================================
Шаг A: Запуск супервизора и демонстрация сигналов
==================================================
Супервизор запущен с PID: 9270. Наблюдайте за логами...
[SUPERVISOR 9270] Супервизор запущен.
[SUPERVISOR 9270] Запуск 3 воркеров...
[SUPERVISOR 9270] Воркер запущен с PID: 9272
[SUPERVISOR 9270] Воркер запущен с PID: 9273
[SUPERVISOR 9270] Воркер запущен с PID: 9274
[WORKER 9272] [CPU fff] Запущен. Режим: heavy
[WORKER 9274] [CPU fff] Запущен. Режим: heavy
[WORKER 9273] [CPU fff] Запущен. Режим: heavy

---> Отправляем SIGUSR1 для переключения в 'легкий' режим
[SUPERVISOR 9270] Получен SIGUSR1
[WORKER 9272] [CPU fff] Переключение в 'легкий' режим (light)
[WORKER 9274] [CPU fff] Переключение в 'легкий' режим (light)
[WORKER 9273] [CPU fff] Переключение в 'легкий' режим (light)

---> Отправляем SIGUSR2 для возврата в 'тяжелый' режим
[SUPERVISOR 9270] Получен SIGUSR2
[WORKER 9273] [CPU fff] Переключение в 'тяжелый' режим (heavy)
[WORKER 9274] [CPU fff] Переключение в 'тяжелый' режим (heavy)
[WORKER 9272] [CPU fff] Переключение в 'тяжелый' режим (heavy)

---> Демонстрация 'graceful reload' через SIGHUP
[SUPERVISOR 9270] Получен SIGHUP. Перезапуск воркеров...
[WORKER 9272] [CPU fff] Получен SIGTERM, завершаю работу...
[WORKER 9274] [CPU fff] Получен SIGTERM, завершаю работу...
[WORKER 9273] [CPU fff] Получен SIGTERM, завершаю работу...
[WORKER 9274] [CPU fff] Работа завершена.
[WORKER 9273] [CPU fff] Работа завершена.
[WORKER 9272] [CPU fff] Работа завершена.
[SUPERVISOR 9270] Запуск 3 воркеров...
[SUPERVISOR 9270] Воркер запущен с PID: 9493
[SUPERVISOR 9270] Воркер запущен с PID: 9494
[SUPERVISOR 9270] Воркер запущен с PID: 9495
[WORKER 9493] [CPU fff] Запущен. Режим: heavy

---> Демонстрация перезапуска при аварийном падении
Принудительно убиваем воркера с PID: 9493
./src/supervisor.sh: line 85:  9493 Killed                  wait -n
[SUPERVISOR 9270] Воркер 9493 завершился с кодом 137.
[SUPERVISOR 9270] Перезапуск воркера...
[SUPERVISOR 9270] Новый воркер запущен с PID: 9539
[WORKER 9539] [CPU fff] Запущен. Режим: heavy
Супервизор должен был его перезапустить.

---> Корректное завершение через SIGTERM
[SUPERVISOR 9270] Получен сигнал завершения. Отправка SIGTERM воркерам...
[WORKER 9494] [CPU fff] Получен SIGTERM, завершаю работу...
[WORKER 9495] [CPU fff] Получен SIGTERM, завершаю работу...
[WORKER 9539] [CPU fff] Получен SIGTERM, завершаю работу...
[WORKER 9539] [CPU fff] Работа завершена.
[WORKER 9494] [CPU fff] Работа завершена.
[WORKER 9495] [CPU fff] Работа завершена.
[SUPERVISOR 9270] Проверка оставшихся воркеров...
[SUPERVISOR 9270] Супервизор завершает работу.
Демонстрация завершена.
```

### Выводы по Части A
Реализация полностью соответствует требованиям. Супервизор успешно управляет воркерами, обрабатывает сигналы для смены режима, перезагрузки конфигурации и корректного завершения. Механизм перезапуска аварийно завершенных дочерних процессов работает исправно.

---

## Часть B: Планирование: nice и CPU-аффинити

### Шаги выполнения
Для демонстрации влияния `nice` на планирование был проведен эксперимент:
1.  Два процесса, активно потребляющие CPU, были запущены одновременно и привязаны к одному ядру CPU (`taskset -c 0`) для создания конкуренции.
2.  С помощью `pidstat` были сняты метрики их производительности.
3.  Эксперимент был повторен, но для второго процесса было установлено значение `nice = +10`, что понижает его приоритет.

### Результаты

**До применения `nice` (равный приоритет):**

```
Linux 6.14.0-33-generic (zhenklchhhPC) 	10/05/2025 	_x86_64_	(12 CPU)

05:02:48 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:02:49 PM  1000      9622   50.00    0.00    0.00   50.00   50.00     0  bash
05:02:49 PM  1000      9623   50.00    0.00    0.00   50.00   50.00     0  bash

05:02:49 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:02:50 PM  1000      9622   50.00    0.00    0.00   50.00   50.00     0  bash
05:02:50 PM  1000      9623   50.00    0.00    0.00   50.00   50.00     0  bash

05:02:50 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:02:51 PM  1000      9622   50.00    0.00    0.00   50.00   50.00     0  bash
05:02:51 PM  1000      9623   50.00    0.00    0.00   50.00   50.00     0  bash

05:02:51 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:02:52 PM  1000      9622   50.00    0.00    0.00   50.00   50.00     0  bash
05:02:52 PM  1000      9623   50.00    0.00    0.00   50.00   50.00     0  bash

05:02:52 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:02:53 PM  1000      9622   50.00    0.00    0.00   50.00   50.00     0  bash
05:02:53 PM  1000      9623   50.00    0.00    0.00   50.00   50.00     0  bash

Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
Average:     1000      9622   50.00    0.00    0.00   50.00   50.00     -  bash
Average:     1000      9623   50.00    0.00    0.00   50.00   50.00     -  bash
```
**После применения `nice = +10` ко второму процессу:**
```
Linux 6.14.0-33-generic (zhenklchhhPC) 	10/05/2025 	_x86_64_	(12 CPU)

05:03:20 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:03:21 PM  1000      9636   99.00    0.00    0.00    0.00   99.00     0  bash
05:03:21 PM  1000      9637    0.00    0.00    0.00    0.00    0.00    10  run.sh

05:03:21 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:03:22 PM  1000      9636  100.00    0.00    0.00    0.00  100.00     0  bash
05:03:22 PM  1000      9637    0.00    0.00    0.00    0.00    0.00    10  run.sh

05:03:22 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:03:23 PM  1000      9636  100.00    0.00    0.00    0.00  100.00     0  bash
05:03:23 PM  1000      9637    0.00    0.00    0.00    0.00    0.00    10  run.sh

05:03:23 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:03:24 PM  1000      9636  100.00    0.00    0.00    0.00  100.00     0  bash
05:03:24 PM  1000      9637    0.00    0.00    0.00    0.00    0.00    10  run.sh

05:03:24 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
05:03:25 PM  1000      9636  100.00    0.00    0.00    0.00  100.00     0  bash
05:03:25 PM  1000      9637    0.00    0.00    0.00    0.00    0.00    10  run.sh

Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
Average:     1000      9636   99.80    0.00    0.00    0.00   99.80     -  bash
Average:     1000      9637    0.00    0.00    0.00    0.00    0.00     -  run.sh
```
### Выводы по Части B
Из результатов `pidstat` видно, что при равном приоритете оба процесса получают примерно одинаковое количество процессорного времени (~50% %CPU). После установки `nice = +10` для второго процесса его доля CPU значительно падает, в то время как процесс с `nice = 0` получает почти всё доступное время. Это наглядно демонстрирует, как планировщик CFS (Completely Fair Scheduler) в Linux распределяет процессорное время пропорционально приоритетам (весам) процессов.

---

## Вопросы для отчёта

**1. Чем процесс отличается от потока в Linux? Где это видно в `ps` и `/proc`?**

*   **Отличие:** Основное отличие — в управлении памятью. **Процесс** имеет собственное, изолированное адресное пространство. **Потоки** (threads) одного процесса разделяют общее адресное пространство (память, файловые дескрипторы), но имеют собственные стеки и регистры. Создание процесса (`fork`) дороже, чем создание потока (`clone`), из-за копирования адресного пространства.
*   **Как увидеть:**
    *   В `ps`: команда `ps -eLf` показывает потоки. У всех потоков одного процесса будет одинаковый `PID`, но разные `LWP` (Light Weight Process ID, он же TID - Thread ID).
    *   В `/proc`: Для процесса с PID `<pid>` существует директория `/proc/<pid>`. Внутри нее есть поддиректория `/proc/<pid>/task/`, где для каждого потока этого процесса создана своя директория с именем, равным его `TID`.

**2. Как `nice` влияет на планирование CFS? Какие есть пределы/исключения?**

*   **Влияние:** Значение `nice` (от -20 до +19) используется планировщиком CFS для определения "веса" процесса. Чем ниже `nice`, тем выше приоритет и вес. CFS распределяет процессорное время пропорционально весам всех активных процессов. Процесс с `nice = -20` получит значительно больше времени, чем процесс с `nice = +19`, при условии, что они оба хотят выполняться.
*   **Пределы и исключения:** `nice` влияет на распределение времени только для обычных политик планирования (`SCHED_NORMAL`). Он не влияет на процессы реального времени (`SCHED_FIFO`, `SCHED_RR`), которые всегда будут вытеснять обычные процессы. Эффект `nice` заметен только при **конкуренции за CPU**. Если процессор простаивает, даже процесс с `nice = +19` получит 100% доступного ему CPU.

**3. Что даёт CPU-аффинити и когда она вредна?**

*   **Что даёт:** CPU-аффинити (привязка процесса/потока к конкретным ядрам CPU) позволяет улучшить производительность за счет повышения эффективности кэширования. Когда процесс всегда выполняется на одном и том же ядре, его данные с большей вероятностью остаются в кэше этого ядра (L1/L2), что уменьшает задержки доступа к памяти. Это критично для высокопроизводительных и real-time приложений.
*   **Когда вредна:** Она может быть вредна, если используется неправильно. Жесткая привязка может привести к **несбалансированной нагрузке**, когда одни ядра перегружены, а другие простаивают. Это мешает системному планировщику, который обычно хорошо справляется с распределением нагрузки. Также, если привязать к одному ядру больше вычислительно-интенсивных потоков, чем оно может обработать, они будут мешать друг другу, вместо того чтобы выполняться параллельно на свободных ядрах.

**4. Чем отличаются `RLIMIT_AS`, `RLIMIT_DATA`, `RLIMIT_RSS`? Почему `RLIMIT_RSS` часто игнорируется?**

*   `RLIMIT_AS` (Address Space): Ограничивает общий объем **виртуальной памяти**, который может запросить процесс. Это самый распространенный и надежный лимит.
*   `RLIMIT_DATA`: Ограничивает размер сегмента данных (heap + BSS), т.е. память, выделенную через `brk()` и `sbrk()`.
*   `RLIMIT_RSS` (Resident Set Size): Ограничивает объем **физической памяти** (ОЗУ), который может занимать процесс.
*   **Почему `RLIMIT_RSS` игнорируется:** Ядру Linux очень сложно точно отслеживать и принудительно ограничивать RSS для отдельного процесса. Причины: общие библиотеки (shared libraries), которые используются многими процессами, но считаются в RSS каждого; страничная подкачка (swapping), когда страницы памяти могут выгружаться на диск и возвращаться. Управление физической памятью — это глобальная задача для ядра, и оно предпочитает использовать другие механизмы, такие как OOM Killer, когда память заканчивается во всей системе. Поэтому поддержка `RLIMIT_RSS` была признана неэффективной и в современных ядрах практически не работает.

**5. Почему возможны зомби и как их избежать при массовых рестартах воркеров?**

*   **Почему возможны:** Процесс-зомби — это завершенный дочерний процесс, запись о котором все еще хранится в таблице процессов. Он существует для того, чтобы родительский процесс мог прочитать его код завершения. Зомби появляются, когда родитель не вызывает системный вызов `wait()` или `waitpid()` для "сбора" информации о завершенном дочернем процессе.
*   **Как избежать:** Родительский процесс **обязан** обрабатывать сигнал `SIGCHLD`, который ядро посылает при завершении дочернего процесса. В обработчике этого сигнала нужно вызывать `wait()` или `waitpid()` в цикле, чтобы собрать статусы всех завершившихся детей. В нашем `supervisor.sh` эту роль выполняет цикл `while` с командой `wait -n`, которая ждет завершения любого дочернего процесса и позволяет супервизору отреагировать.

**6. Чем отличается «graceful shutdown» от «graceful reload/restart»? Какие последовательности безопасны?**

*   **Graceful Shutdown (корректное завершение):** Процесс получает сигнал (`SIGTERM`/`SIGINT`), перестает принимать новые задачи, завершает обработку текущих, освобождает ресурсы (закрывает файлы, сетевые соединения) и после этого завершается. Цель — не потерять данные и чисто выйти.
    *   *Безопасная последовательность:* `kill -SIGTERM <pid>` -> Процесс завершает текущие дела -> `exit()`.
*   **Graceful Reload/Restart (мягкая перезагрузка):** Процесс (обычно супервизор) обновляет свою конфигурацию или код без остановки обслуживания. Старые воркеры завершают обработку своих задач, в то время как новые воркеры, запущенные с новой конфигурацией, уже принимают новые задачи. Цель — нулевое время простоя (zero downtime).
    *   *Безопасная последовательность:* `kill -SIGHUP <supervisor_pid>` -> Супервизор запускает новых воркеров -> Супервизор посылает `SIGTERM` старым воркерам -> Старые воркеры завершают работу.

**7. Как повлияют контейнерные лимиты (cgroup v2) на наблюдаемые метрики процесса?**

Контейнерные лимиты, реализуемые через `cgroups v2`, обеспечивают более строгую и предсказуемую изоляцию, чем `ulimit`.
*   **CPU:** Если для cgroup установлен лимит CPU (`cpu.max`), то процессы внутри него не смогут потреблять больше указанной квоты. В `pidstat` или `top` мы увидим, что `%CPU` упирается в этот лимит. Также может появиться ненулевое значение `%steal` (stolen time), показывающее, сколько времени процесс хотел бы работать, но не мог из-за ограничений.
*   **Память:** Лимит `memory.max` работает гораздо надежнее, чем `RLIMIT_RSS`. Когда процессы в cgroup достигают этого лимита, будет вызван OOM Killer, который убьет процесс **внутри этого cgroup**, а не случайный процесс в системе. В `/proc/<pid>/status` появятся поля, относящиеся к cgroup, а метрики RSS будут строго ограничены. Это делает поведение системы под нагрузкой гораздо более предсказуемым.
