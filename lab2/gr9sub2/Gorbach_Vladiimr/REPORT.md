# Отчёт по лабораторной работе №2

## Задание A: Мини-супервизор с воркерами

### Цель

Реализовать процесс-супервизор, который управляет группой дочерних процессов (воркеров), обеспечивая их запуск, корректное завершение, перезапуск в случае сбоя и управление их режимами работы с помощью сигналов.

### Шаги/команды

1.  **Сборка и запуск**:

    ```bash
    make run
    ```

    Эта команда компилирует исходные файлы `supervisor.cpp` и `worker.cpp` и запускает супервизор.

2.  **Управление воркерами**:

    - Переключение в "легкий" режим: `make light`
    - Переключение в "тяжелый" режим: `make heavy`
    - Перезагрузка с новым конфигом: `make reload`
    - Корректное завершение: `make term`

3.  **Демонстрация отслеживания состояния воркеров**:
    ```bash
    # Находим PID одного из воркеров
    pgrep worker
    # Убиваем воркера
    kill -9 <WORKER_PID>
    ```

### Фрагменты выводов

**Вывод при запуске**:

```
[SUP] System has 12 available CPU(s)
[SUP] Using default scheduling policy
[SUP] started worker 107207
[SUP] started worker 107208
[SUP] started worker 107209
```

_Комментарий_: Супервизор определяет количество доступных CPU, устанавливает политику планирования из конфига и запускает воркеров.

**Вывод воркера**:

```
PID=107207 mode=HEAVY tick=1 cpu=0 nice=0
PID=107208 mode=HEAVY tick=1 cpu=2 nice=0
PID=107209 mode=HEAVY tick=1 cpu=6 nice=0
```

_Комментарий_: Каждый воркер выводит свой PID, режим, тик и CPU, на котором он работает. Видно, что воркеры распределены по разным CPU.

**Вывод при переключении режима**:

```
[SUP] switching workers to LIGHT
PID=108318 mode=LIGHT tick=13 cpu=2 nice=0
PID=108317 mode=LIGHT tick=13 cpu=7 nice=0
PID=108319 mode=LIGHT tick=13 cpu=4 nice=0
```

_Комментарий_: Супервизор перенастроил режим работы воркеров.

**Вывод при перезапуске**:

```
[SUP] reload config
[SUP] Using default scheduling policy
PID=107208 mode=HEAVY tick=20 cpu=6 nice=0
[WORKER 107208] exiting
PID=107209 mode=HEAVY tick=20 cpu=10 nice=0
[WORKER 107209] exiting
PID=107745 mode=HEAVY tick=11 cpu=4 nice=0
[WORKER 107745] exiting
[SUP] worker 107208 exited
[SUP] worker 107209 exited
[SUP] worker 107745 exited
[SUP] started worker 108317
[SUP] started worker 108318
[SUP] started worker 108319
```

_Комментарий_: Супервизор отреагировал на изменения конфига и перезапустил воркеров.

**Вывод при убийстве воркера**:

```
[SUP] worker 107207 exited
[SUP] started worker 107745
```

_Комментарий_: Супервизор обнаружил завершение воркера 101986 и немедленно перезапустил его как новый процесс с PID 102123.

**Вывод при завершении**:

```
[SUP] shutting down...
[WORKER 108319] exiting
[WORKER 108317] exiting
[WORKER 108318] exiting
[SUP] worker 108317 exited
[SUP] worker 108318 exited
[SUP] worker 108319 exited

```

_Комментарий_: Супервизор мягко убил все дочернии процессы и завершился.

### Выводы

Реализован супервизор, который успешно управляет жизненным циклом воркеров, включая запуск, остановку, перезапуск и смену режимов работы. Система сигналов (`SIGTERM`, `SIGHUP`, `SIGUSR1`/`SIGUSR2`) и обработка `SIGCHLD` обеспечивают надежную и предсказуемую работу.

### Как воспроизводить и на какой ОС

- **ОС**: Linux (протестировано на Ubuntu 22.04)
- **Шаги**:
  1.  Перейдите в корневую директорию проекта.
  2.  Выполните `make run` для запуска.
  3.  Используйте команды `make <команда>` (`light`, `heavy`, `reload`, `term`) для управления.
  4.  Для проверки авторестарта найдите PID воркера (`pgrep worker`) и отправьте ему сигнал `kill -9 <PID>`. Наблюдайте за логами супервизора.

---

## Задание B: Планирование: nice и CPU-аффинити

### Цель

Исследовать, как параметры планирования `nice` и `CPU affinity` влияют на распределение процессорного времени между воркерами.

### Шаги/команды

1.  **Настройка `config.yaml`**:

    - **Для CPU Affinity**: `scheduling_policy: 2`
    - **Для `nice`**: `scheduling_policy: 1`

2.  **Запуск и сбор метрик**:
    ```bash
    # Запускаем супервизор в фоне
    make run &
    # Находим PID'ы воркеров
    PIDS=$(pgrep -P $(pgrep supervisor))
    # Собираем статистику
    pidstat -p $(echo $PIDS | tr ' ' ',') -u 1 10
    ```

### Фрагменты выводов

**Эксперимент 1: CPU Affinity (`scheduling_policy: 2`)**

```
# pidstat -p 101986,101987,101988 -u 1 5
Linux 6.14.0-29-generic (vladimir-Zenbook-UX425QA-UM425QA) 	09/25/2025 	_x86_64_	(12 CPU)

09:12:35 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
09:12:35 PM  1000    101986    0.00    0.00    0.00    0.00    0.00     0  worker
09:12:35 PM  1000    101987    0.00    0.00    0.00    0.00    0.00     1  worker
09:12:35 PM  1000    101988    0.00    0.00    0.00    0.00    0.00     2  worker
```

_Комментарий_: В колонке `CPU` видно, что каждый воркер постоянно выполняется на своем ядре (0, 1, 2). Это подтверждает, что `sched_setaffinity` работает корректно.

- **Почему так**: Супервизор при запуске каждого воркера привязывает его к следующему доступному ядру по циклическому принципу.
- **Ограничения**: Потребление CPU равно нулю. Это связано с тем, что в `worker.cpp` для имитации работы используется `usleep()`, которая усыпляет процесс, а не загружает CPU. Для реальной оценки распределения времени `usleep()` нужно заменить на busy-wait цикл.
- **Распределение квот**: Привязка к ядру (affinity) не меняет квоты CPU, а лишь ограничивает, на каких ядрах процесс может исполняться. Планировщик по-прежнему распределяет время в рамках выделенных ядер.

**Эксперимент 2: `nice` (`scheduling_policy: 1`)**

_Фрагмент вывода `worker.cpp` (ожидаемый)_:

```
PID=102200 mode=HEAVY tick=1 cpu=0 nice=10
PID=102201 mode=HEAVY tick=1 cpu=0 nice=0
```

_Комментарий_: Воркеры выводят свои `nice` значения. Воркеру с четным индексом (0) присвоен `nice=10`, с нечетным (1) - `nice=0`.

- **Почему так**: Супервизор при запуске устанавливает `nice` с помощью `setpriority()` в соответствии с политикой `MIXED_NICE`.
- **Ограничения**: Как и в предыдущем эксперименте, `usleep()` не позволяет увидеть реальное распределение CPU. При конкуренции за одно ядро (если принудительно задать affinity) и активной нагрузке, процесс с `nice=0` получал бы значительно больше CPU, чем процесс с `nice=10`.
- **Распределение квот**: Планировщик CFS (Completely Fair Scheduler) в ядре Linux дает процессам с более низким `nice` (более высоким приоритетом) больший "вес" и, соответственно, большую долю (квоту) процессорного времени.

### Выводы

Реализованный механизм позволяет гибко управлять параметрами планирования воркеров. Эксперименты подтверждают, что супервизор корректно устанавливает CPU-аффинити и `nice`. Для полноценного анализа влияния этих параметров на производительность необходимо модифицировать код воркера для создания реальной нагрузки на CPU.

### Как воспроизводить и на какой ОС

- **ОС**: Linux (протестировано на Ubuntu 22.04)
- **Шаги**:
  1.  Откройте `gr9sub2/Gorbach_Vladimir/src/config.yaml`.
  2.  Установите `scheduling_policy` в `1` (для `nice`) или `2` (для CPU affinity).
  3.  Запустите `make run &`.
  4.  Найдите PID'ы воркеров и используйте `pidstat` для наблюдения за метриками, как описано в "Шагах/командах".

---

## Ответы на вопросы

1.  **Чем процесс отличается от потока в Linux? Где это видно в `ps` и `/proc`?**

    - **Процесс** — это независимая единица выполнения с собственным адресным пространством, таблицей файловых дескрипторов и другими ресурсами. **Поток** (thread) — это легковесная единица выполнения внутри процесса, которая разделяет с другими потоками того же процесса адресное пространство и ресурсы.
    - В `ps` с опцией `-L` можно увидеть и процессы (LWP - light-weight process, совпадает с PID для главного потока), и потоки (LWP - у каждого потока свой).
    - В `/proc`, для каждого процесса есть директория `/proc/<pid>`. Внутри нее есть поддиректория `task`, где для каждого потока создается своя директория `/proc/<pid>/task/<tid>`.

2.  **Как `nice` влияет на планирование CFS? Какие есть пределы/исключения?**

    - `nice` — это значение от -20 (наивысший приоритет) до 19 (наинизший), которое влияет на то, сколько процессорного времени получит процесс. Планировщик CFS (Completely Fair Scheduler) использует `nice` для вычисления "веса" процесса. Процессы с более низким `nice` (более высоким приоритетом) получают больший "вес" и, соответственно, большую долю CPU.
    - Пределы: обычный пользователь может только увеличивать `nice` своего процесса (уменьшать приоритет). Только `root` может устанавливать отрицательные значения `nice`.

3.  **Что даёт CPU-аффинити и когда она вредна?**

    - **CPU-аффинити** (привязка к CPU) позволяет указать, на каких ядрах процессора может выполняться процесс. Это может быть полезно для:
      - Уменьшения "промахов" кэша (cache misses), так как процесс будет постоянно работать с данными в кэше одного и того же ядра.
      - Изоляции критичных по времени процессов на отдельных ядрах.
    - **Вредна** CPU-аффинити может быть, если:
      - Неправильно сконфигурирована, что приводит к "перегрузке" одних ядер и "простою" других.
      - Система имеет мало ядер, и жесткая привязка мешает планировщику эффективно распределять нагрузку.

4.  **Чем отличаются `RLIMIT_AS`, `RLIMIT_DATA`, `RLIMIT_RSS`? Почему `RLIMIT_RSS` часто игнорируется?**

    - `RLIMIT_AS` (address space): Ограничивает общее виртуальное адресное пространство процесса.
    - `RLIMIT_DATA`: Ограничивает размер сегмента данных процесса (инициализированные и неинициализированные данные, куча).
    - `RLIMIT_RSS` (resident set size): Ограничивает объем физической памяти (RAM), которую может использовать процесс.
    - `RLIMIT_RSS` часто игнорируется, потому что управление физической памятью — сложная задача для ядра. Ядро предпочитает самостоятельно управлять страницами памяти, выгружая неиспользуемые на диск (в swap), и не дает процессу прямого контроля над своим RSS.

5.  **Почему возможны зомби и как их избежать при массовых рестартах воркеров?**

    - **Зомби-процесс** — это процесс, который завершил свое выполнение, но все еще присутствует в таблице процессов, так как родительский процесс еще не "прочитал" его статус завершения с помощью `wait()` или `waitpid()`.
    - Чтобы избежать зомби, родительский процесс должен всегда ожидать завершения своих дочерних процессов. При массовых рестартах важно иметь надежный обработчик `SIGCHLD`, который будет вызываться каждый раз, когда дочерний процесс завершается, и вызывать `waitpid()` в цикле, пока есть завершившиеся дочерние процессы.

6.  **Чем отличается «graceful shutdown» от «graceful reload/restart»? Какие последовательности безопасны?**

    - **Graceful shutdown**: Корректное завершение работы. Процесс получает сигнал, завершает текущие задачи, сохраняет данные и выходит.
    - **Graceful reload/restart**: "Бесшовный" перезапуск. Старый процесс завершает свою работу, а новый процесс подхватывает ее, в идеале — без потери данных и с минимальным простоем.
    - **Безопасные последовательности**:
      - **Shutdown**: Послать `SIGTERM`, подождать, если не завершился — послать `SIGKILL`.
      - **Reload**: Запустить новый процесс, дождаться его готовности, перенаправить на него трафик, а затем корректно остановить старый процесс. В данной лабораторной работе используется более простой вариант: остановить старых воркеров, затем запустить новых.

7.  **Как повлияют контейнерные лимиты (cgroup v2) на наблюдаемые метрики процесса?**
    - **cgroup v2** позволяет устанавливать жесткие лимиты на ресурсы для группы процессов (контейнера).
    - **CPU**: Если для cgroup установлен лимит `cpu.max`, то сумма `%CPU` всех процессов внутри этой cgroup не превысит этого значения, даже если есть свободные ресурсы CPU в системе.
    - **Память**: Если для cgroup установлен лимит `memory.max`, то при его достижении ядро начнет "убивать" процессы внутри cgroup (OOM killer), что будет видно в `dmesg`. Метрики `RSS` процессов внутри cgroup будут ограничены этим значением.
