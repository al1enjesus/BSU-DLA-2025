# Отчёт по лабораторной работе №2

## Задание A: Мини-супервизор и процессы-воркеры

### Цель

Создать процесс-супервизор, который управляет группой дочерних процессов (воркеров): запускает их, завершает корректно, перезапускает при сбое и меняет их режим работы с помощью сигналов.

### Шаги/команды

1. **Сборка и старт**:

   ```bash
   make run
   ```

   Данная команда компилирует `supervisor.cpp` и `worker.cpp`, затем запускает процесс супервизора.

2. **Управление режимами воркеров**:

   * Переключение в «лёгкий» режим: `make light`
   * Переключение в «тяжёлый» режим: `make heavy`
   * Перезапуск с подгрузкой нового конфига: `make reload`
   * Завершение работы: `make term`

3. **Проверка реакции на сбой воркера**:

   ```bash
   # Узнаём PID одного из воркеров
   pgrep worker
   # Завершаем воркер принудительно
   kill -9 <WORKER_PID>
   ```

### Примеры выводов

**Логи при старте**:

```
[SUP] System has 12 available CPU(s)
[SUP] Using default scheduling policy
[SUP] started worker 107207
[SUP] started worker 107208
[SUP] started worker 107209
```

*Пояснение*: Супервизор получает количество доступных CPU, применяет политику планирования из конфигурации и запускает воркеров.

**Выводы воркеров**:

```
PID=107207 mode=HEAVY tick=1 cpu=0 nice=0
PID=107208 mode=HEAVY tick=1 cpu=2 nice=0
PID=107209 mode=HEAVY tick=1 cpu=6 nice=0
```

*Пояснение*: Каждый воркер сообщает свой PID, текущий режим, номер тика и CPU. Видно распределение по ядрам.

**Логи при смене режима**:

```
[SUP] switching workers to LIGHT
PID=108318 mode=LIGHT tick=13 cpu=2 nice=0
PID=108317 mode=LIGHT tick=13 cpu=7 nice=0
PID=108319 mode=LIGHT tick=13 cpu=4 nice=0
```

*Пояснение*: Супервизор переключил режим работы всех процессов.

**Перезапуск воркеров**:

```
[SUP] reload config
[SUP] Using default scheduling policy
PID=107208 mode=HEAVY tick=20 cpu=6 nice=0
[WORKER 107208] exiting
PID=107209 mode=HEAVY tick=20 cpu=10 nice=0
[WORKER 107209] exiting
PID=107745 mode=HEAVY tick=11 cpu=4 nice=0
[WORKER 107745] exiting
[SUP] worker 107208 exited
[SUP] worker 107209 exited
[SUP] worker 107745 exited
[SUP] started worker 108317
[SUP] started worker 108318
[SUP] started worker 108319
```

*Пояснение*: После обновления конфигурации супервизор остановил старые процессы и создал новые.

**Реакция на завершение воркера**:

```
[SUP] worker 107207 exited
[SUP] started worker 107745
```

*Пояснение*: Супервизор заметил исчезновение процесса и сразу поднял замену с новым PID.

**Корректное завершение**:

```
[SUP] shutting down...
[WORKER 108319] exiting
[WORKER 108317] exiting
[WORKER 108318] exiting
[SUP] worker 108317 exited
[SUP] worker 108318 exited
[SUP] worker 108319 exited

```

*Пояснение*: Супервизор аккуратно завершил дочерние процессы и сам остановился.

### Итог

Разработанный супервизор корректно управляет жизненным циклом воркеров: запуском, остановкой, перезапуском и сменой режимов. Использование сигналов (`SIGTERM`, `SIGHUP`, `SIGUSR1`/`SIGUSR2`) и обработка `SIGCHLD` гарантируют устойчивую работу.

### Воспроизведение и окружение

* **ОС**: Linux (тестировалось на Ubuntu 22.04)
* **Действия**:

  1. Перейти в директорию проекта.
  2. Выполнить `make run`.
  3. Управлять через `make <команда>` (`light`, `heavy`, `reload`, `term`).
  4. Для проверки автоперезапуска: определить PID воркера (`pgrep worker`), затем завершить его (`kill -9 <PID>`) и проследить за логами супервизора.

---

## Задание B: Планирование — nice и CPU-аффинити

### Цель

Изучить, как параметры `nice` и привязка к ядрам (CPU affinity) влияют на распределение процессорного времени между воркерами.

### Шаги/команды

1. **Редактирование `config.yaml`**:

   * Для проверки CPU-аффинити: `scheduling_policy: 2`
   * Для проверки `nice`: `scheduling_policy: 1`

2. **Запуск эксперимента**:

   ```bash
   # Запускаем супервизор в фоне
   make run &
   # Получаем PID'ы воркеров
   PIDS=$(pgrep -P $(pgrep supervisor))
   # Снимаем метрики
   pidstat -p $(echo $PIDS | tr ' ' ',') -u 1 10
   ```

### Примеры выводов

**Тест 1: CPU Affinity (`scheduling_policy: 2`)**

```
# pidstat -p 101986,101987,101988 -u 1 5
Linux 6.14.0-29-generic (denis-Zenbook-UX425QA-UM425QA) 	09/25/2025 	_x86_64_	(12 CPU)

09:12:35 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
09:12:35 PM  1000    101986    0.00    0.00    0.00    0.00    0.00     0  worker
09:12:35 PM  1000    101987    0.00    0.00    0.00    0.00    0.00     1  worker
09:12:35 PM  1000    101988    0.00    0.00    0.00    0.00    0.00     2  worker
```

*Пояснение*: В колонке `CPU` видно, что каждый процесс закреплён за своим ядром (0, 1, 2). Это подтверждает правильность работы `sched_setaffinity`.

* **Причина**: супервизор закрепляет воркеров по ядрам по циклу.
* **Особенность**: загрузка CPU равна нулю, так как воркер использует `usleep()` вместо вычислительной нагрузки.
* **Вывод**: affinity ограничивает процесс только набором ядер, но не меняет квоты распределения времени.

**Тест 2: `nice` (`scheduling_policy: 1`)**

*Фрагмент вывода воркеров*:

```
PID=102200 mode=HEAVY tick=1 cpu=0 nice=10
PID=102201 mode=HEAVY tick=1 cpu=0 nice=0
```

*Пояснение*: Чётному воркеру задан `nice=10`, нечётному — `nice=0`.

* **Причина**: супервизор устанавливает приоритет с помощью `setpriority()` по схеме `MIXED_NICE`.
* **Ограничение**: из-за `usleep()` реальная конкуренция за CPU не проявляется. При активной нагрузке воркер с `nice=0` получил бы большую долю CPU.
* **Вывод**: планировщик CFS даёт процессам с меньшим `nice` больший вес и больше времени процессора.

### Итог

Механизм позволяет изменять стратегию планирования воркеров. Эксперименты подтвердили корректность установки affinity и nice. Для более наглядной картины сто́ит заменить `usleep()` на вычислительную нагрузку.

### Воспроизведение и окружение

* **ОС**: Linux (тестировалось на Ubuntu 22.04)
* **Действия**:

  1. Открыть `gr9sub2/Lebedev_Denis/src/config.yaml`.
  2. Задать `scheduling_policy = 1` (для `nice`) или `2` (для affinity).
  3. Запустить `make run &`.
  4. Использовать `pidstat` для анализа.

---

## Ответы на вопросы

1.  **Чем процесс отличается от потока в Linux? Где это видно в ps и /proc?**

    - **Процесс** — это независимая единица выполнения с собственным адресным пространством, таблицей файловых дескрипторов и другими ресурсами. **Поток** (thread) — это легковесная единица выполнения внутри процесса, которая разделяет с другими потоками того же процесса адресное пространство и ресурсы.
    - В `ps` с опцией `-L` можно увидеть и процессы (LWP - light-weight process, совпадает с PID для главного потока), и потоки (LWP - у каждого потока свой).
    - В `/proc`, для каждого процесса есть директория `/proc/<pid>`. Внутри нее есть поддиректория `task`, где для каждого потока создается своя директория `/proc/<pid>/task/<tid>`.

2.  **Как nice влияет на планирование CFS? Какие есть пределы/исключения?**

    - `nice` — это значение от -20 (наивысший приоритет) до 19 (наинизший), которое влияет на то, сколько процессорного времени получит процесс. Планировщик CFS (Completely Fair Scheduler) использует `nice` для вычисления "веса" процесса. Процессы с более низким `nice` (более высоким приоритетом) получают больший "вес" и, соответственно, большую долю CPU.
    - Пределы: обычный пользователь может только увеличивать `nice` своего процесса (уменьшать приоритет). Только `root` может устанавливать отрицательные значения `nice`.

3.  **Что даёт CPU-аффинити и когда она вредна?**

    - **CPU-аффинити** (привязка к CPU) позволяет указать, на каких ядрах процессора может выполняться процесс. Это может быть полезно для:
      - Уменьшения "промахов" кэша (cache misses), так как процесс будет постоянно работать с данными в кэше одного и того же ядра.
      - Изоляции критичных по времени процессов на отдельных ядрах.
    - **Вредна** CPU-аффинити может быть, если:
      - Неправильно сконфигурирована, что приводит к "перегрузке" одних ядер и "простою" других.
      - Система имеет мало ядер, и жесткая привязка мешает планировщику эффективно распределять нагрузку.

4.  **Чем отличаются `RLIMIT_AS`, `RLIMIT_DATA`, `RLIMIT_RSS`? Почему `RLIMIT_RSS` часто игнорируется?**

    - `RLIMIT_AS` (address space): Ограничивает общее виртуальное адресное пространство процесса.
    - `RLIMIT_DATA`: Ограничивает размер сегмента данных процесса (инициализированные и неинициализированные данные, куча).
    - `RLIMIT_RSS` (resident set size): Ограничивает объем физической памяти (RAM), которую может использовать процесс.
    - `RLIMIT_RSS` часто игнорируется, потому что управление физической памятью — сложная задача для ядра. Ядро предпочитает самостоятельно управлять страницами памяти, выгружая неиспользуемые на диск (в swap), и не дает процессу прямого контроля над своим RSS.

5.  **Почему возможны зомби и как их избежать при массовых рестартах воркеров?**

    - **Зомби-процесс** — это процесс, который завершил свое выполнение, но все еще присутствует в таблице процессов, так как родительский процесс еще не "прочитал" его статус завершения с помощью `wait()` или `waitpid()`.
    - Чтобы избежать зомби, родительский процесс должен всегда ожидать завершения своих дочерних процессов. При массовых рестартах важно иметь надежный обработчик `SIGCHLD`, который будет вызываться каждый раз, когда дочерний процесс завершается, и вызывать `waitpid()` в цикле, пока есть завершившиеся дочерние процессы.

6.  **Чем отличается «graceful shutdown» от «graceful reload/restart»? Какие последовательности безопасны?**

    - **Graceful shutdown**: Корректное завершение работы. Процесс получает сигнал, завершает текущие задачи, сохраняет данные и выходит.
    - **Graceful reload/restart**: "Бесшовный" перезапуск. Старый процесс завершает свою работу, а новый процесс подхватывает ее, в идеале — без потери данных и с минимальным простоем.
    - **Безопасные последовательности**:
      - **Shutdown**: Послать `SIGTERM`, подождать, если не завершился — послать `SIGKILL`.
      - **Reload**: Запустить новый процесс, дождаться его готовности, перенаправить на него трафик, а затем корректно остановить старый процесс. В данной лабораторной работе используется более простой вариант: остановить старых воркеров, затем запустить новых.

7.  **Как повлияют контейнерные лимиты (cgroup v2) на наблюдаемые метрики процесса?**
    - **cgroup v2** позволяет устанавливать жесткие лимиты на ресурсы для группы процессов (контейнера).
    - **CPU**: Если для cgroup установлен лимит `cpu.max`, то сумма `%CPU` всех процессов внутри этой cgroup не превысит этого значения, даже если есть свободные ресурсы CPU в системе.
    - **Память**: Если для cgroup установлен лимит `memory.max`, то при его достижении ядро начнет "убивать" процессы внутри cgroup (OOM killer), что будет видно в `dmesg`. Метрики `RSS` процессов внутри cgroup будут ограничены этим значением.
